# -*- coding: utf-8 -*-
"""App_with_Last_CamemBERT_X_SVC_Mathias_Haberli_Sophie_Daya_Kaggle.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jLrbGdF4AsPvoafhlRQL9Mv6ZnSlScP0

# IMPORT THE DATA
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Adjust the path based on where you've stored the files
training_data = '/content/drive/MyDrive/Colab Notebooks/2024_Data_science/Final_Project_Kaggle_Competition/training_data.csv'
unlabelled_data = '/content/drive/MyDrive/Colab Notebooks/2024_Data_science/Final_Project_Kaggle_Competition/unlabelled_test_data.csv'
sample_submission = '/content/drive/MyDrive/Colab Notebooks/2024_Data_science/Final_Project_Kaggle_Competition/sample_submission.csv'

training_data_pd = pd.read_csv(training_data)
unlabelled_data_pd = pd.read_csv(unlabelled_data)
sample_submission_pd = pd.read_csv(sample_submission)

display(training_data_pd)
display(unlabelled_data_pd)
display(sample_submission_pd)

"""# BERT"""

import pandas as pd
import string

# Function to clean text
def clean_text(text):
    # Convert text to lowercase
    text = text.lower()
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Remove extra whitespace
    text = " ".join(text.split())
    return text

# Apply cleaning function to the sentence column in both datasets
training_data_pd['sentence'] = training_data_pd['sentence'].apply(clean_text)
unlabelled_data_pd['sentence'] = unlabelled_data_pd['sentence'].apply(clean_text)

# Display the first few rows of the cleaned data to confirm changes
print(training_data_pd.head())
print(unlabelled_data_pd.head())

"""
!pip install transformers
!pip install torch
"""
!pip install --no-cache-dir accelerate==0.29.3
!pip install --no-cache-dir transformers[torch]==4.40.1

import accelerate
print(accelerate.__version__)

import accelerate
import transformers
print("Accelerate version:", accelerate.__version__)
print("Transformers version:", transformers.__version__)

from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from transformers import Trainer, TrainingArguments

# Load the tokenizer and model
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=6)  # Assuming 6 labels for CEFR levels A1, A2, B1, B2, C1, C2

pip install optuna

pip install transformers datasets

pip install transformers

#59%

from transformers import CamembertTokenizer, CamembertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback
from sklearn.model_selection import train_test_split
from datasets import Dataset, load_metric
import numpy as np

# Load the tokenizer
tokenizer = CamembertTokenizer.from_pretrained('camembert-base')

# Function to encode data
def encode_data(tokenizer, df):
    texts = df['sentence'].tolist()
    labels = df['difficulty'].map({'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4, 'C2': 5}).tolist()
    encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=128)
    return Dataset.from_dict({
        'input_ids': encodings['input_ids'],
        'attention_mask': encodings['attention_mask'],
        'labels': labels
    })

# Split the DataFrame into training and validation sets
train_data, val_data = train_test_split(training_data_pd, test_size=0.10, random_state=42)

# Tokenize and prepare datasets
train_dataset = encode_data(tokenizer, train_data)
val_dataset = encode_data(tokenizer, val_data)

# Load the model
model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=6)

# Metric for evaluation
def compute_metrics(eval_pred):
    metric = load_metric("accuracy")
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    accuracy = metric.compute(predictions=predictions, references=labels)
    return {"accuracy": accuracy['accuracy']}

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=6,  # Slight adjustment if needed based on results
    per_device_train_batch_size=22,  # A bit smaller to adjust gradient updates
    warmup_steps=500,  # Adjust based on the total number of steps (num_epochs * total_data / batch_size)
    weight_decay=0.01,  # Good for regularization
    logging_dir='./logs',
    logging_steps=50,  # Less frequent to reduce logging overhead
    learning_rate=15e-5,  # Adjusted for potentially better convergence
    fp16=True,  # Make sure your hardware supports FP16 for this to be effective
    evaluation_strategy="epoch",  # Change to steps if you need more frequent evaluation
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model='accuracy',
    greater_is_better=True
)

# Initialize the Trainer with added compute_metrics for dynamic metric calculation
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,  # Ensure metrics are computed during evaluation
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]

)

# Train the model
trainer.train()

pip install streamlit

import streamlit as st
from transformers import CamembertTokenizer
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd

# Load the tokenizer (make sure it's available or loaded)
tokenizer = CamembertTokenizer.from_pretrained('camembert-base')

# Define the dataset class as shown in your snippet
class SimpleDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        return {key: val[idx] for key, val in self.encodings.items()}

    def __len__(self):
        return len(self.encodings.input_ids)

# Function to predict the difficulty level of a sentence
def predict_difficulty(sentence):
    # Tokenize the input sentence
    encodings = tokenizer([sentence], truncation=True, padding=True, max_length=512, return_tensors="pt")
    dataset = SimpleDataset(encodings)

    # Predict using the trainer
    predictions = trainer.predict(dataset)
    predicted_label = np.argmax(predictions.predictions, axis=1)[0]

    # Mapping from labels to difficulties
    difficulty_levels = {0: 'A1', 1: 'A2', 2: 'B1', 3: 'B2', 4: 'C1', 5: 'C2'}
    return difficulty_levels[predicted_label]

# Streamlit interface
st.title('French Text Difficulty Prediction')
input_sentence = st.text_area("Enter a French sentence to predict its difficulty level:")

if st.button('Predict Difficulty'):
    if input_sentence:
        difficulty = predict_difficulty(input_sentence)
        st.write(f'The predicted difficulty level of the entered sentence is: **{difficulty}**')
    else:
        st.write("Please enter a valid sentence.")

!streamlit run app.py

"""from transformers import CamembertTokenizer
import numpy as np

# Load the tokenizer
tokenizer = CamembertTokenizer.from_pretrained('camembert-base')

# Tokenize the sentences from the 'sentence' column, specifying max_length
max_length = 512  # Typical max length for BERT-based models
unlabelled_encodings = tokenizer(list(unlabelled_data_pd['sentence']), truncation=True, padding=True, max_length=max_length, return_tensors="pt")

# Assuming 'trainer' is your trained model's Trainer instance
from torch.utils.data import Dataset, DataLoader

class SimpleDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        return {key: val[idx] for key, val in self.encodings.items()}

    def __len__(self):
        return len(self.encodings.input_ids)

# Create a dataset from the tokenized data
dataset = SimpleDataset(unlabelled_encodings)

# Predict using the trainer
predictions = trainer.predict(dataset)
predicted_labels = np.argmax(predictions.predictions, axis=1)

# Define a mapping from numeric labels to categories
difficulty_levels = {0: 'A1', 1: 'A2', 2: 'B1', 3: 'B2', 4: 'C1', 5: 'C2'}
predicted_difficulties = [difficulty_levels[label] for label in predicted_labels]

import pandas as pd

# Create a DataFrame for submission
submission_df = pd.DataFrame({
    'id': unlabelled_data_pd['id'],
    'difficulty': predicted_difficulties
})

# Save the DataFrame to a CSV file for submission
submission_df.to_csv('/content/drive/My Drive/Colab Notebooks/2024_Data_science/Final_Project_Kaggle_Competition/submission_CamemBERT_12.csv', index=False)

!ls -lh

!rm -rf /content/logs

!rm -rf /content/results

!rm -rf /content/sample_data

!ls -lh
"""